{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mnist' from '/home/hminle/Github/weekly-ml-projects/variational-auto-encoder/mnist.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import mnist; importlib.reload(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parser arguments\n",
    "parser = {\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"no_cuda\": False,\n",
    "    \"seed\": 1,\n",
    "    \"log_interval\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse arguments for  model\n",
    "args = argparse.Namespace(**parser) # parse arguments\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define seed for torch\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_workers: use core of machine\n",
    "# pin_memory is useful for GPU\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "# Download data from datasets on the Internet\n",
    "# \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    mnist.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # pictures with 28x28 --> 784\n",
    "        self.fc1 = nn.Linear(784, 400) #400 neural\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x)) # input --> fc1 --> relu --> h1\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if args.cuda:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        # z --> fc3 --> relu --> fc4 --> sigmoid\n",
    "        # Sigmoid will normalize data into 0 - 1 interval\n",
    "        h3 = self.relu(self.fc3(z)) \n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    # every model need to have a forward def --> how data flow\n",
    "    def forward(self, x):\n",
    "        # input = batchsize x 28 x28 --> batchsize x 784\n",
    "        # flatten input and put it into encode, -1 is to keep the original batch size\n",
    "        # mu = mean ...\n",
    "        # logvar = log variational\n",
    "        mu, logvar = self.encode(x.view(-1, 784)) \n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "# if we use cuda, model.cuda will load parameter into GPU for us\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # recon_x: output of decoder\n",
    "    # x: input of encoder\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # KLD = KL\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    # loss = BCE + KLD --> need to check with paper\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of VAE (\n",
       "  (fc1): Linear (784 -> 400)\n",
       "  (fc21): Linear (400 -> 20)\n",
       "  (fc22): Linear (400 -> 20)\n",
       "  (fc3): Linear (20 -> 400)\n",
       "  (fc4): Linear (400 -> 784)\n",
       "  (relu): ReLU ()\n",
       "  (sigmoid): Sigmoid ()\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and train def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we use pretrained model, and need to update only fc4:\n",
    "#optimizer = optim.Adam(model.fc4.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # This has any effect only on modules such as Dropout or BatchNorm.\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Variable for back-prop, it wraps input + gradient \n",
    "        # of loss function with respect to input\n",
    "        data = Variable(data)\n",
    "        if args.cuda:\n",
    "            data = data.cuda() # if cuda True, transfer data to GPU\n",
    "        # create buffer for grad parameters --> must have for pytorch\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        # Because we init data = Variable --> loss and other parameters = Variable\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        # Auto Gradient, not update weights yet\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        # Update weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    # Different from def train --> using model.eval\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        # Init data with Variable(volatile=True) \n",
    "        # --> other parameters do not need to bring gradient\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 548.967590\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 310.296082\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 235.784332\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 224.120712\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 208.667755\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 207.585968\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 208.356522\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 201.860794\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 192.739700\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 196.974609\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 192.592743\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 176.308838\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 181.238693\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 172.837067\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 171.217438\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 164.194183\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 159.862610\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 160.299576\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 157.998581\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 160.794434\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 155.614105\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 155.336838\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 149.538940\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 150.300491\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 147.928757\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 146.100357\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 149.449493\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 148.985458\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 140.447037\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 143.553421\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 141.248047\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 139.762756\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 137.494385\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 139.359222\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 136.170166\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 135.285980\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 134.138947\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 136.852005\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 135.686493\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 133.650558\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 135.745316\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 133.173996\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 130.662964\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 132.734604\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 133.634567\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 128.811127\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 126.485046\n",
      "====> Epoch: 1 Average loss: 166.0929\n",
      "====> Test set loss: 128.1614\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 125.624016\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 124.775848\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 132.897888\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 128.872421\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 126.013664\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 126.819435\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 124.363190\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 124.653709\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 123.312859\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 127.184799\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 124.846237\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 129.660538\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 124.683540\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 121.740265\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 130.155151\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 125.546371\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 120.889755\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 121.509918\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 120.098457\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 122.037338\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 123.397850\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 118.258736\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 120.935684\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 120.264160\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 121.237343\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 123.111900\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 125.034149\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 118.343369\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 116.242584\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 115.542435\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 117.887032\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 117.214600\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 125.163406\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 116.330536\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 120.176895\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 122.268517\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 116.819466\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 118.230881\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 115.277901\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 116.185699\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 120.938065\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 118.163330\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 115.116081\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 117.549843\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 117.308197\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 118.323151\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 117.930626\n",
      "====> Epoch: 2 Average loss: 121.4843\n",
      "====> Test set loss: 115.4199\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 118.528763\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 121.438782\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 117.533081\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 117.711609\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 117.557495\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 116.623306\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 114.198013\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 120.388268\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 114.234650\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 118.234718\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 117.403450\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 116.221985\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 114.693802\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 113.043724\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 111.297592\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 114.377197\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 115.773544\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 117.028831\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 113.377884\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 110.617409\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 111.271393\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 116.676567\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 114.131943\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 116.648201\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 111.514626\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 117.688492\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 112.248512\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 113.637337\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 111.638351\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 113.283936\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.056335\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 110.143600\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 117.549080\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 115.108292\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 109.864899\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 116.121391\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 115.350800\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 110.230408\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 112.306641\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 112.166351\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 110.272873\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 112.448860\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 114.794670\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 113.548058\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 110.526199\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 109.645111\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 114.134796\n",
      "====> Epoch: 3 Average loss: 114.3967\n",
      "====> Test set loss: 111.7071\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 113.818420\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 117.843132\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 113.694427\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 111.953476\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 112.739143\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 112.326576\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 109.942291\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 113.317566\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 113.202698\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 112.264587\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.246307\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 105.882019\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 116.431290\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 106.953087\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 114.574814\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 109.230591\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 113.689079\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 108.102318\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 108.704796\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 109.763275\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 109.477951\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 109.276459\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 109.724228\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 110.391640\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 110.110931\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 112.013428\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 109.677826\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 114.054428\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 113.690002\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 110.699356\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 109.829529\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 109.610840\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 111.941757\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 112.909142\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 107.726471\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 113.769920\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 113.204620\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 107.482574\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 111.491096\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 112.374130\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 114.667297\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 113.866821\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 114.959190\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 116.151764\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 109.634460\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 110.117599\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 112.632439\n",
      "====> Epoch: 4 Average loss: 111.4877\n",
      "====> Test set loss: 109.6397\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 111.530792\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 113.933212\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 107.932220\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 110.685951\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 114.855423\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 113.092659\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 112.379669\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 112.379700\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 110.591049\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 109.261475\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 109.770309\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 112.059784\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 108.506721\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 112.246155\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 110.224548\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 108.904732\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 111.146820\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 108.260551\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 105.472717\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 111.324013\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 110.043594\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 106.028748\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 110.603668\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 105.635086\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 113.742920\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 107.420929\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 113.375206\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 109.037910\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 110.515259\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 110.617737\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 109.152611\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 106.922180\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 109.122108\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 109.086052\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 109.462158\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 115.670792\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 114.154968\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 110.607483\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 108.111328\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 109.262253\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 110.368309\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 111.908691\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 112.440987\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 107.823906\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 114.675140\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.135429\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 109.020805\n",
      "====> Epoch: 5 Average loss: 109.7851\n",
      "====> Test set loss: 108.4056\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 112.202179\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 109.820343\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 110.855789\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 108.295135\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 111.349426\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 108.216446\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 114.610748\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 110.656456\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 108.324432\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 103.533012\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 105.993813\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 110.430191\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 108.022217\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 112.849617\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 109.498566\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 111.684311\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 110.681305\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 107.022339\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 109.727684\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 110.435104\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 106.828384\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 106.904472\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 107.121437\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 112.626434\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 112.012794\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 104.719849\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 108.453201\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 105.524704\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 105.275116\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 108.726105\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 108.485344\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 109.274483\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 111.925316\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 104.073120\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 113.165649\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 108.296135\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 102.543854\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 112.796700\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 106.083557\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 111.914467\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.187538\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 110.490082\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 106.541901\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 109.710617\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 106.456512\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 111.516083\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 111.853584\n",
      "====> Epoch: 6 Average loss: 108.5990\n",
      "====> Test set loss: 107.4241\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 111.860107\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 108.659737\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 105.318352\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 110.487694\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 112.214439\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 105.387398\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 105.360878\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 108.265160\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 107.710083\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 104.335251\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 108.087540\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 107.778152\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 107.631592\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 107.036362\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 111.096603\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 109.503380\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 110.620956\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 106.389488\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 107.861145\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 105.299911\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 105.289268\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 106.571075\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 105.906708\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 112.055496\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 111.576752\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 109.114540\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 111.084496\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 108.867538\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 105.521561\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 108.980377\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 109.856857\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 105.774742\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 105.340439\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 107.496803\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 110.996017\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 105.917480\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 108.084160\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 103.940727\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 108.215714\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 106.301132\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 109.672493\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 106.235229\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 106.406952\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 110.944466\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 103.909264\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 109.155411\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 107.012665\n",
      "====> Epoch: 7 Average loss: 107.7718\n",
      "====> Test set loss: 106.8288\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 106.402534\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 106.702782\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 108.447571\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 108.295578\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 111.222000\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 104.715530\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 107.402985\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 110.145309\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 107.583870\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 106.572952\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 102.777756\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 109.244568\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 107.784050\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 108.131248\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 110.533363\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 106.220276\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 108.912476\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 108.202629\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 103.246620\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 106.178604\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 106.610764\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 107.353905\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 108.887039\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 109.543274\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 109.321152\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 103.348709\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 111.290283\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 106.688774\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 105.533463\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 109.352585\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.193832\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 108.808914\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 107.457748\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 108.667686\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 106.841507\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 103.683510\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 106.631180\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 104.678810\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 107.545151\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 103.778656\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 104.489334\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 109.747849\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 106.945061\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 103.513611\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 105.615051\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 108.235199\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 104.901672\n",
      "====> Epoch: 8 Average loss: 107.1703\n",
      "====> Test set loss: 106.2751\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 106.511314\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 101.694038\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 106.468384\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 108.231461\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 108.221825\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 107.330063\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 104.107010\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 108.512039\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 105.507286\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 107.708023\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 105.488380\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 107.828354\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 107.306366\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 104.604042\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 105.623932\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 105.747658\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 105.124115\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 108.934799\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 106.681564\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 107.485039\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 105.040497\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 107.434792\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 105.917351\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 105.296272\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 108.175339\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 109.069443\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 104.825439\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 107.945740\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 106.747971\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 104.093521\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 103.485199\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 109.592163\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 108.279221\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 107.111588\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 106.490196\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 101.891968\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 101.392334\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 105.053772\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 108.405708\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 108.534592\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 106.004448\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 107.371895\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 108.425674\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 107.863220\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 110.737854\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 109.918503\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 110.057480\n",
      "====> Epoch: 9 Average loss: 106.6682\n",
      "====> Test set loss: 106.2228\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 108.766792\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 110.066620\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 105.878372\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 107.315208\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 105.686752\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 108.440491\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 103.691193\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 107.669472\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 102.918465\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 106.882858\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 103.721451\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 109.612579\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 104.026337\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 107.045349\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 107.318993\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 106.369987\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 106.359085\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 106.737343\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 107.161407\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 109.446762\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 108.830154\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 107.614258\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 107.364883\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 105.214691\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 109.034554\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 103.338684\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 109.363152\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 109.765877\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 103.918625\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 109.274422\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 106.790741\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 106.589912\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 108.857834\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 111.732162\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 103.627533\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 107.720505\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 106.276031\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 105.941544\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 103.776779\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 104.770020\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 108.059914\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 105.023987\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 108.493408\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 106.951920\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 105.189690\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 107.963470\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 108.927521\n",
      "====> Epoch: 10 Average loss: 106.2782\n",
      "====> Test set loss: 105.7358\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
