{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parser arguments\n",
    "# nz: size of latent variable\n",
    "parser = {\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"dataroot\": \"./\",\n",
    "    \"workers\": 2,\n",
    "    \"batchSize\": 64,\n",
    "    \"imageSize\": 32,\n",
    "    \"nz\": 100,\n",
    "    \"ngf\": 64,\n",
    "    \"ndf\": 64,\n",
    "    \"epochs\": 7,\n",
    "    \"lr\": 0.0002,\n",
    "    \"beta1\": 0.5,\n",
    "    \"cuda\": False,\n",
    "    \"ngpu\": 1,\n",
    "    \"netG\": '',\n",
    "    \"netD\": '',\n",
    "    \"outf\": './output',\n",
    "    \"manualSeed\": 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse arguments for  model\n",
    "args = argparse.Namespace(**parser) # parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create output\n",
    "try:\n",
    "    os.makedirs(args.outf)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  7\n"
     ]
    }
   ],
   "source": [
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", args.manualSeed)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = dset.CIFAR10(root=args.dataroot, download=False,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(args.imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataLoader \n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batchSize,shuffle=True, num_workers=int(args.workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngpu = int(args.ngpu)\n",
    "nz = int(args.nz) #     z_dim\n",
    "ngf = int(args.ngf) # g_dim??\n",
    "ndf = int(args.ndf) # d_dim??\n",
    "nc = 3 # Image channels R, G, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "# m: model\n",
    "# Init weights and bias for G and D\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        #self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf * 1, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 1),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # state size. (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        #gpu_ids = None\n",
    "        #if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "        #    gpu_ids = range(self.ngpu)\n",
    "        #return nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init weight G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU (inplace)\n",
      "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): Tanh ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = _netG()\n",
    "netG.apply(weights_init)\n",
    "#if opt.netG != '':\n",
    "#    netG.load_state_dict(torch.load(opt.netG))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 8 x 8\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        #gpu_ids = None\n",
    "        #if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "        #    gpu_ids = range(self.ngpu)\n",
    "        #output = nn.parallel.data_parallel(self.main, input, gpu_ids)\n",
    "        #return output.view(-1, 1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU (0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU (0.2, inplace)\n",
      "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (9): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = _netD()\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input = torch.FloatTensor(args.batchSize, 3, args.imageSize, args.imageSize)\n",
    "#noise = torch.FloatTensor(args.batchSize, nz, 1, 1)\n",
    "#fixed_noise = torch.FloatTensor(args.batchSize, nz, 1, 1).normal_(0, 1)\n",
    "#label = torch.FloatTensor(args.batchSize)\n",
    "#real_label = 1\n",
    "#fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input = Variable(input)\n",
    "#label = Variable(label)\n",
    "#noise = Variable(noise)\n",
    "#fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "D_optimizer = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "G_optimizer = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for i, (data, _) in enumerate(dataloader):\n",
    "        # Update net D\n",
    "        ## 1. Train with real \n",
    "        real_label = Variable(torch.ones(data.size(0)))\n",
    "        fake_label = Variable(torch.zeros(data.size(0)))\n",
    "        \n",
    "        data_input = Variable(data)\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        D_real_output = netD(data_input)\n",
    "        errD_real = criterion(D_real_output, real_label)\n",
    "        errD_real.backward()\n",
    "        D_x = D_real_output.data.mean()\n",
    "        \n",
    "        ## 2. Train with fake\n",
    "        z = Variable(torch.randn(data.size(0), nz, 1, 1))\n",
    "        G_fake_output = netG(z)\n",
    "        D_fake_output = netD(G_fake_output.detach())\n",
    "        errD_fake = criterion(D_fake_output, fake_label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = D_fake_output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # Update net G\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        D_fake_output_for_G = netD(G_fake_output)\n",
    "        errG = criterion(D_fake_output_for_G, real_label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = D_fake_output_for_G.data.mean()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, args.epochs, i, len(dataloader),\n",
    "                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(data, \n",
    "                              '%s/real_samples.png' % args.outf)\n",
    "            fake = netG(z)\n",
    "            vutils.save_image(fake.data,\n",
    "                              '%s/fake_samples_epoch_%03d.png' % (args.outf, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/7][0/782] Loss_D: 2.2511 Loss_G: 1.9060 D(x): 0.4320 D(G(z)): 0.6970 / 0.1812\n",
      "[0/7][1/782] Loss_D: 1.6067 Loss_G: 2.3086 D(x): 0.4786 D(G(z)): 0.5104 / 0.1124\n",
      "[0/7][2/782] Loss_D: 1.1202 Loss_G: 2.5234 D(x): 0.5530 D(G(z)): 0.3640 / 0.0991\n",
      "[0/7][3/782] Loss_D: 0.9129 Loss_G: 2.5712 D(x): 0.6553 D(G(z)): 0.3371 / 0.0897\n",
      "[0/7][4/782] Loss_D: 0.8061 Loss_G: 2.7044 D(x): 0.7424 D(G(z)): 0.3663 / 0.0771\n",
      "[0/7][5/782] Loss_D: 0.9128 Loss_G: 2.8198 D(x): 0.6902 D(G(z)): 0.3543 / 0.0712\n",
      "[0/7][6/782] Loss_D: 0.8338 Loss_G: 2.9883 D(x): 0.7274 D(G(z)): 0.3636 / 0.0620\n",
      "[0/7][7/782] Loss_D: 0.8118 Loss_G: 3.2244 D(x): 0.7040 D(G(z)): 0.3304 / 0.0451\n",
      "[0/7][8/782] Loss_D: 0.8056 Loss_G: 3.3952 D(x): 0.7524 D(G(z)): 0.3710 / 0.0413\n",
      "[0/7][9/782] Loss_D: 0.7459 Loss_G: 3.6645 D(x): 0.7513 D(G(z)): 0.3313 / 0.0315\n",
      "[0/7][10/782] Loss_D: 0.8713 Loss_G: 3.7833 D(x): 0.7385 D(G(z)): 0.3998 / 0.0271\n",
      "[0/7][11/782] Loss_D: 1.0306 Loss_G: 4.0137 D(x): 0.6951 D(G(z)): 0.4360 / 0.0224\n",
      "[0/7][12/782] Loss_D: 0.8341 Loss_G: 4.3897 D(x): 0.7148 D(G(z)): 0.3479 / 0.0157\n",
      "[0/7][13/782] Loss_D: 0.6514 Loss_G: 4.3409 D(x): 0.7083 D(G(z)): 0.2272 / 0.0155\n",
      "[0/7][14/782] Loss_D: 0.7352 Loss_G: 4.3725 D(x): 0.7305 D(G(z)): 0.2961 / 0.0155\n",
      "[0/7][15/782] Loss_D: 0.8185 Loss_G: 4.6517 D(x): 0.7228 D(G(z)): 0.3336 / 0.0127\n",
      "[0/7][16/782] Loss_D: 0.7097 Loss_G: 4.9856 D(x): 0.7700 D(G(z)): 0.3236 / 0.0084\n",
      "[0/7][17/782] Loss_D: 0.6175 Loss_G: 4.9181 D(x): 0.7279 D(G(z)): 0.2053 / 0.0087\n",
      "[0/7][18/782] Loss_D: 0.6182 Loss_G: 4.8978 D(x): 0.7609 D(G(z)): 0.2478 / 0.0094\n",
      "[0/7][19/782] Loss_D: 0.6945 Loss_G: 5.1518 D(x): 0.7474 D(G(z)): 0.2843 / 0.0078\n",
      "[0/7][20/782] Loss_D: 0.6992 Loss_G: 5.3816 D(x): 0.7855 D(G(z)): 0.3202 / 0.0060\n",
      "[0/7][21/782] Loss_D: 0.5262 Loss_G: 5.4967 D(x): 0.8190 D(G(z)): 0.2501 / 0.0056\n",
      "[0/7][22/782] Loss_D: 0.8125 Loss_G: 5.2851 D(x): 0.7058 D(G(z)): 0.3007 / 0.0074\n",
      "[0/7][23/782] Loss_D: 0.8180 Loss_G: 5.6402 D(x): 0.7904 D(G(z)): 0.3786 / 0.0048\n",
      "[0/7][24/782] Loss_D: 0.6546 Loss_G: 5.8406 D(x): 0.7987 D(G(z)): 0.3101 / 0.0038\n",
      "[0/7][25/782] Loss_D: 0.8511 Loss_G: 5.1160 D(x): 0.5967 D(G(z)): 0.1741 / 0.0085\n",
      "[0/7][26/782] Loss_D: 0.6069 Loss_G: 5.0041 D(x): 0.7401 D(G(z)): 0.2209 / 0.0083\n",
      "[0/7][27/782] Loss_D: 0.5274 Loss_G: 4.9856 D(x): 0.7582 D(G(z)): 0.1760 / 0.0087\n",
      "[0/7][28/782] Loss_D: 0.4882 Loss_G: 5.0795 D(x): 0.8048 D(G(z)): 0.1755 / 0.0086\n",
      "[0/7][29/782] Loss_D: 0.4652 Loss_G: 5.3656 D(x): 0.8545 D(G(z)): 0.2159 / 0.0063\n",
      "[0/7][30/782] Loss_D: 0.3813 Loss_G: 5.2614 D(x): 0.8133 D(G(z)): 0.1375 / 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/hminle/anaconda2/envs/pydata35/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0968062e7eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-512d9c998940>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mD_fake_output_for_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_fake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake_output_for_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_fake_output_for_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hminle/anaconda2/envs/pydata35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    144\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hminle/anaconda2/envs/pydata35/lib/python3.5/site-packages/torch/autograd/_functions/pointwise.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hminle/anaconda2/envs/pydata35/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
